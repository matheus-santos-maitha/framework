<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Análise: utils_vm.py</title>
    <link rel="stylesheet" href="styles.css">
    <style>
        /* Estilo para o link de citação */
        a.citation-link {
            font-size: 0.8em;
            vertical-align: super;
            text-decoration: none;
            color: var(--secondary-color, #3498db);
            font-weight: bold;
            margin-left: 4px;
            border: 1px solid var(--light-gray, #ecf0f1);
            padding: 1px 5px;
            border-radius: 4px;
        }
        a.citation-link:hover {
            text-decoration: none;
            background-color: var(--secondary-color, #3498db);
            color: white;
        }
    </style>
</head>
<body>

    <nav>
        <a href="index.html" class="nav-header-link"><h2>Módulos do Framework Jumper</h2></a>
        
        <h4>Infraestrutura e Ambiente</h4>
        <ul>
            <li><a href="utils_vm.html">utils_vm.py</a></li>
            <li><a href="environment.html">environment.py</a></li>
            <li><a href="utils.html">utils.py</a></li>
        </ul>
        <h4>Conectores e Acesso a Dados</h4>
        <ul>
            <li><a href="connector.html">connector.py</a></li>
            <li><a href="postgres_utils.html">postgres_utils.py</a></li>
            <li><a href="bigquery_utils.html">bigquery_utils.py</a></li>
        </ul>
        <h4>Lógica ETL Principal</h4>
        <ul>
            <li><a href="migracao_bronze.html">migracao_tabelas_bronze.py</a></li>
            <li><a href="migracao_silver.html">migracao_tabelas_silver.py</a></li>
        </ul>
        <h4>Bibliotecas Utilitárias</h4>
        <ul>
            <li><a href="manage_dataframe.html">manage_dataframe.py</a></li>
            <li><a href="transformation.html">transformation.py</a></li>
            <li><a href="validation.html">validation.py</a></li>
        </ul>
    </nav>

    <main>
        <article>
            <header>
                <h1>Análise Técnica Aprofundada: Módulo <code>utils_vm.py</code></h1>
            </header>

            <section>
                <h2>Visão Geral da Arquitetura</h2>
                <p>O código <code>utils_vm.py</code> é um dos módulos de código reutilizáveis que compõem a "Fábrica de DAG's".<a href="sources/Fabrica_de_DAGs.pdf#page=2" class="citation-link" target="_blank">[fonte]</a> Sua principal função é abstrair e padronizar a criação de recursos de infraestrutura (clusters Dataproc) e a definição de tarefas (jobs PySpark), servindo como o motor principal para as DAGs geradas dinamicamente.</p>
            </section>

            <section>
                <h2>Destaque de Engenharia: Seleção Dinâmica de Recursos</h2>
                <p>é importante destacar o valor de engenharia da função <code>CLUSTER_CONFIG(**kwargs)</code>. Esta função é a peça central que permite a seleção dinâmica do "shape" da máquina para cada pipeline.</p>
                <dl>
                    <dt>Funcionamento</dt>
                    <dd>Os parâmetros que definem a infraestrutura do cluster, como <code>vm_type</code> (o "shape" da máquina) e <code>num_workers</code>, são lidos do arquivo de configuração YAML da DAG e passados para esta função. Isso permite que cada pipeline seja executado com os recursos computacionais precisamente dimensionados para sua necessidade.<a href="sources/Fabrica_de_DAGs.pdf#page=4" class="citation-link" target="_blank">[fonte]</a></dd>
                    
                    <dt>Valor para a Trade Master</dt>
                    <dd>Esta flexibilidade é um grande valor de engenharia, pois permite que a equipe otimize a performance e os custos de cada processo de ETL individualmente, sem precisar alterar o código base do framework. A funcionalidade que estava "escondida" é um dos pilares da eficiência e escalabilidade da solução.</dd>
                </dl>
            </section>

            <section>
                <h2>Análise Funcional Detalhada</h2>
                <dl>
                    <dt>Centralização de Configurações via Airflow Variables</dt>
                    <dd>O script inicia obtendo configurações globais como <code>PROJECT_ID</code> e <code>REGION</code> através de <code>Variable.get()</code>. Esta abordagem desvincula as configurações do código, permitindo que sejam gerenciadas diretamente na interface do Airflow.</dd>

                    <dt>Função <code>job_dag(...)</code></dt>
                    <dd>Atua como um construtor padronizado para jobs PySpark. Sua função é garantir que todas as tarefas submetidas ao Dataproc sigam uma estrutura consistente.
                        <ul>
                            <li><strong>Multi-Database:</strong> A inclusão de múltiplos drivers JDBC equipa o framework com a capacidade de se conectar a diversas fontes de dados, como Oracle e Postgre.<a href="sources/Framework_Engenharia_de_Dados.pdf#page=1" class="citation-link" target="_blank">[fonte]</a></li>
                            <li><strong>Identificadores Únicos:</strong> A função utiliza <code>random_string()</code> para gerar um <code>job_id</code> único para cada execução, evitando conflitos na API do Dataproc.</li>
                            <li><strong>Parametrização:</strong> Permite a passagem de argumentos (<code>args</code>) diretamente para o script PySpark, o que é fundamental para a reutilização de scripts genéricos.<a href="sources/Fabrica_de_DAGs.pdf#page=2" class="citation-link" target="_blank">[fonte]</a></li>
                        </ul>
                    </dd>
                    
                    <dt>Dicionário <code>default_args</code></dt>
                    <dd>Define um conjunto de parâmetros padrão para todas as DAGs geradas pela fábrica. Isso impõe consistência e boas práticas, como políticas de retentativas (<code>retries</code>) e o proprietário (<code>owner</code>) de cada DAG.</dd>
                </dl>
            </section>

        </article>
    </main>
</body>
</html>